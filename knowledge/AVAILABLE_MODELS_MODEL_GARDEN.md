# Models supported by Model Garden

> **Open Source Licenses**
>
> The Model Garden might include a link to the public repository where openly available models are hosted. To confirm that you comply with the license terms, review any applicable model, dataset, or software licenses. Also confirm that you comply with any restrictions in the license (such as use-based restrictions included in the CreativeML Open RAIL-M license).

## List of Google's first-party models
The following table lists the Google's first-party models that are available in Model Garden:

| Model name | Modality | Description |
| :--- | :--- | :--- |
| **Gemini 2.5 Flash (Preview)** | Language, audio, vision | Thinking model that is designed to balance price and performance. |
| **Gemini 2.5 Pro (Preview)** | Language, audio, vision | Thinking model with next generation features and improved capabilities. |
| **Gemini 2.0 Flash** | Language, audio, vision | The workhorse model for all daily tasks and features enhanced performance and supports real-time Live API. |
| **Gemini 2.0 Flash-Lite** | Language, audio, vision | The fastest and most cost efficient Flash model. It provides better quality than 1.5 for the same price and speed. |
| **Imagen for Image Generation** | Vision | Create studio-grade images at scale using text prompts. You can also use this model to upscale images. |
| **Imagen for Editing and Customization** | Vision | Edit or use few-shot learning to create studio-grade images at scale using base images and text prompts, or using reference images and text prompts. |
| **Vertex Image Segmentation (Preview)** | Vision | Use text prompts or draw scribbles to segment an image. Image segmentation lets you, for example, detect objects, remove the background of an image, or segment the foreground of an image. |
| **Imagen for Captioning & VQA** | Language | Generates a relevant description for a given image. |
| **Embeddings for Text** | Language | Generates vectors based on text, which can be used for downstream tasks like semantic search, text classification, and sentiment analysis. |
| **Embeddings for Multimodal** | Vision | Generates vectors based on images, which can be used for downstream tasks like image classification and image search. |
| **Chirp 2** | Speech | Chirp 2 is a multilingual automatic speech recognition (ASR) model developed by Google that transcribes speech (speech-to-text). Compared to the first generation of Chirp models, Chirp 2 provides improved accuracy and speed, and offers new capabilities like word-level timestamps, model adaptation, and speech translation. |

## List of managed models available in Model Garden
Models are offered as managed APIs on Vertex AI Model Garden (also known as model as a service).

## Partner models available in Model Garden
The following table lists the partner models available in Model Garden:

| Model name | Modality | Description |
| :--- | :--- | :--- |
| **Claude Opus 4.5** | Language, Vision | The next generation of Anthropic's most intelligent model, Claude Opus 4.5 is an industry leader across coding, agents, computer use, and enterprise workflows. |
| **Claude Sonnet 4.5** | Language, Vision | Anthropic's mid-sized model for powering real-world agents, with capabilities in coding, computer use, cybersecurity, and working with office files like spreadsheets. |
| **Claude Opus 4.1** | Language, Vision | An industry leader for coding. It delivers sustained performance on long-running tasks that require focused effort and thousands of steps, significantly expanding what AI agents can solve. Ideal for powering frontier agent products and features. |
| **Claude Haiku 4.5** | Language, Vision | Claude Haiku 4.5 delivers near-frontier performance for a wide range of use cases, and stands out as one of the best coding models in the worldâ€“with the right speed and cost to power free products and high-volume user experiences. |
| **Claude Opus 4** | Language, Vision | Claude Opus 4 delivers sustained performance on long-running tasks that require focused effort and thousands of steps, significantly expanding what AI agents can solve. |
| **Claude Sonnet 4** | Language, Vision | Anthropic's mid-size model with superior intelligence for high-volume uses, such as coding, in-depth research, and agents. |
| **Anthropic's Claude 3.5 Sonnet v2** | Language, Vision | The upgraded Claude 3.5 Sonnet is a state-of-the-art model for real-world software engineering tasks and agentic capabilities. Claude 3.5 Sonnet delivers these advancements at the same price and speed as its predecessor. |
| **Anthropic's Claude 3.5 Haiku** | Language, Vision | Claude 3.5 Haiku, the next generation of Anthropic's fastest and most cost-effective model, is optimal for use cases where speed and affordability matter. |
| **Anthropic's Claude 3 Haiku** | Language | Anthropic's fastest vision and text model for near-instant responses to basic queries, meant for seamless AI experiences mimicking human interactions. |
| **Anthropic's Claude 3.5 Sonnet** | Language | Claude 3.5 Sonnet outperforms Anthropic's Claude 3 Opus on a wide range of Anthropic's evaluations with the speed and cost of Anthropic's mid-tier model, Claude 3 Sonnet. |
| **Jamba 1.5 Large (Preview)** | Language | AI21 Labs's Jamba 1.5 Large is designed for superior quality responses, high throughput, and competitive pricing compared to other models in its size class. |
| **Jamba 1.5 Mini (Preview)** | Language | AI21 Labs's Jamba 1.5 Mini is well balanced across quality, throughput, and low cost. |
| **Mistral Medium 3** | Language | Mistral Medium 3 is a versatile model designed for a wide range of tasks, including programming, mathematical reasoning, understanding long documents, summarization, and dialogue. |
| **Mistral OCR (25.05)** | Language, Vision | Mistral OCR (25.05) is an Optical Character Recognition API for document understanding. The model comprehends each element of documents such as media, text, tables, and equations. |
| **Mistral Small 3.1 (25.03)** | Language | Mistral Small 3.1 (25.03) is the latest version of Mistral's Small model, featuring multimodal capabilities and extended context length. |
| **Mistral Large (24.11)** | Language | Mistral Large (24.11) is the next version of the Mistral Large (24.07) model now with improved reasoning and function calling capabilities. |
| **Codestral 2** | Language, Code | Codestral 2 is Mistral's code generation specialized model built specifically for high-precision fill-in-the-middle (FIM) completion that helps developers write and interact with code through a shared instruction and completion API endpoint. |
| **Codestral (25.01)** | Code | A cutting-edge model that's designed for code generation, including fill-in-the-middle and code completion. |

## Open models available in Model Garden
The following table lists the open models available in Model Garden:

| Model name | Modality | Description |
| :--- | :--- | :--- |
| **DeepSeek-OCR** | Language, Vision | A comprehensive Optical Character Recognition (OCR) model that analyzes and understands complex documents. It excels at challenging OCR tasks. |
| **DeepSeek R1 (0528)** | Language | DeepSeek's latest version of the DeepSeek R1 model. |
| **DeepSeek-V3.1** | Language | DeepSeek's hybrid model that supports both thinking mode and non-thinking mode. |
| **gpt-oss 120B** | Language | A 120B model that offers high performance on reasoning tasks. |
| **gpt-oss 20B** | Language | A 20B model optimized for efficiency and deployment on consumer and edge hardware. |
| **Kimi K2 Thinking** | Language | An open-source thinking agent model that reasons step-by-step and uses tools to solve complex problems. |
| **Llama 3.1** | Language | A collection of multilingual LLMs optimized for multilingual dialogue use cases and outperform many of the available open source and closed chat models on common industry benchmarks.<br>Llama 3.1 405B is generally available (GA).<br>Llama 3.1 8B and Llama 3.1 70B are in Preview. |
| **Llama 3.2 (Preview)** | Language, Vision | A medium-sized 90B multimodal model that can support image reasoning, such as chart and graph analysis as well as image captioning. |
| **Llama 3.3** | Language | Llama 3.3 is a text-only 70B instruction-tuned model that provides enhanced performance relative to Llama 3.1 70B and to Llama 3.2 90B when used for text-only applications. Moreover, for some applications, Llama 3.3 70B approaches the performance of Llama 3.1 405B. |
| **Llama 4 Maverick 17B-128E** | Language, Vision | The largest and most capable Llama 4 model that has coding, reasoning, and image capabilities. Llama 4 Maverick 17B-128E is a multimodal model that uses the Mixture-of-Experts (MoE) architecture and early fusion. |
| **Llama 4 Scout 17B-16E** | Language, Vision | Llama 4 Scout 17B-16E delivers state-of-the-art results for its size class, outperforming previous Llama generations and other open and proprietary models on several benchmarks. Llama 4 Scout 17B-16E is a multimodal model that uses the Mixture-of-Experts (MoE) architecture and early fusion. |
| **MiniMax M2** | Language, Code | Designed for agentic and code-related tasks with strong capabilities in planning and executing complex tool-calling tasks. |
| **Qwen3 235B** | Language | An open-weight model with a "hybrid thinking" capability to switch between methodical reasoning and rapid conversation. |
| **Qwen3 Coder** | Language, Code | An open-weight model developed for advanced software development tasks. |
| **Qwen3-Next-80B Instruct** | Language, Code | A model from the Qwen3-Next family of models, specialized for following specific commands. |
| **Qwen3-Next-80B Thinking** | Language, Code | A model from the Qwen3-Next family of models, specialized for complex problem-solving and deep reasoning. |

## List of models with open source tuning or serving recipes in Model Garden
The following table lists the OSS models that support open source tuning or serving recipes in Model Garden:

| Model name | Modality | Description |
| :--- | :--- | :--- |
| **Llama 4** | Language, Vision | A family of multimodal models that use the Mixture-of-Experts (MoE) architecture and early fusion. |
| **Llama 3.3** | Language | The Meta Llama 3.3 multilingual large language model (LLM) is a pretrained and instruction tuned generative model in 70B (text in/text out). |
| **Flux** | Vision | A 12 billion parameter rectified flow transformer model that generates high-quality images from text descriptions. |
| **Prompt Guard** | Language | Guardrail LLM inputs against jailbreaking techniques and indirect injections. |
| **Llama 3.2** | Language | A collection of multilingual large language models that are pretrained and instruction-tuned generative models in 1B and 3B sizes. |
| **Llama 3.2-Vision** | Language, Vision | A collection of multimodal large language models that are pretrained and instruction-tuned image reasoning generative models in 11B and 90B sizes. |
| **Llama Guard 3** | Language | A Llama-3.1-8B pretrained model that has been fine-tuned for content safety classification. |
| **Qwen2** | Language | Deploy Qwen2, a foundation large language model series. |
| **Phi-3** | Language | Deploy Phi-3, a foundation large language model series. |
| **E5** | Language | Deploy E5, a text embedding model series. |
| **Instant ID** | Language, Vision | Deploy Instant ID, an identity preserving, text-to-image generation model. |
| **Llama 3** | Language | Explore and build with Meta's Llama 3 models (8B, 70B, 405B) on Vertex AI. |
| **Gemma 3n** | Language, Vision, Audio | Open weight models (E2B, E4B) that are built from the same research and technology used to create Google's Gemini models. |
| **Gemma 3** | Language, Vision | Open weight models (1B text-only, 4B, 12B, 27B) that are built from the same research and technology used to create Google's Gemini models. |
| **Gemma 2** | Language | Open weight models (9B, 27B) that are built from the same research and technology used to create Google's Gemini models. |
| **Gemma** | Language | Open weight models (2B, 7B) that are built from the same research and technology used to create Google's Gemini models. |
| **CodeGemma** | Language | Open weight models (2B, 7B) designed for code generation and code completion that are built from the same research and technology used to create Google's Gemini models. |
| **PaliGemma 2** | Language, Vision | Open weight 3B, 10B and 28B models designed for image captioning tasks and visual question and answering tasks that's built from the same research and technology used to create Google's Gemini models. |
| **PaliGemma** | Language, Vision | Open weight 3B model designed for image captioning tasks and visual question and answering tasks that's built from the same research and technology used to create Google's Gemini models. |
| **ShieldGemma 2** | Language, Vision | Open weight 4B model trained on Gemma 3's 4B IT checkpoint for image safety classification across key categories that takes in images and outputs safety labels per policy. |
| **TxGemma** | Language | Open weight models (2B, 9B, 27B) designed for therapeutic development that are built upon Gemma 2. |
| **MedGemma** | Language, Vision | Open weight models (4B, 27B) designed for performance on medical text and image comprehension. |
| **MedSigLIP** | Language, Vision | Open weight model (400M parameter vision encoder and 400M parameter text encoder) designed to encode medical images and text into a common embedding space. |
| **T5Gemma** | Language | Open weight encoder-decoder models (2B-2B, 9B-9B, 9B-2B, S-S, B-B, L-L, M-L, XL-XL) that are built from the same research and technology used to create Google's Gemini models. |
| **Vicuna v1.5** | Language | Deploy Vicuna v1.5 series models, which are foundation models fine-tuned from LLama2 for text generation. |
| **NLLB** | Language | Deploy nllb series models for multi-language translation. |
| **Mistral-7B** | Language | Deploy Mistral-7B, a foundational model for text generation. |
| **BioGPT** | Language | Deploy BioGPT, a text generative model for the biomedical domain. |
| **BiomedCLIP** | Language, Vision | Deploy BiomedCLIP, a multimodal foundation model for the biomedical domain. |
| **ImageBind** | Language, Vision, Audio | Deploy ImageBind, a foundational model for multimodal embedding. |
| **DITO** | Language, Vision | Finetune and deploy DITO, a multimodal foundation model for open vocabulary object detection tasks. |
| **OWL-ViT v2** | Language, Vision | Deploy OWL-ViT v2, a multimodal foundation model for open vocabulary object detection tasks. |
| **FaceStylizer (Mediapipe)** | Vision | A generative pipeline to transform human face images to a new style. |
| **Llama 2** | Language | Finetune and deploy Meta's Llama 2 foundation models (7B, 13B, 70B) on Vertex AI. |
| **Code Llama** | Language | Deploy Meta's Code Llama foundation models (7B, 13B, 34B) on Vertex AI. |
| **Falcon-instruct** | Language | Finetune and deploy Falcon-instruct models (7B, 40B) by using PEFT. |
| **OpenLLaMA** | Language | Finetune and deploy OpenLLaMA models (3B, 7B, 13B) by using PEFT. |
| **T5-FLAN** | Language | Finetune and deploy T5-FLAN (base, small, large). |
| **BERT** | Language | Finetune and deploy BERT by using PEFT. |
| **BART-large-cnn** | Language | Deploy BART, a transformer encoder-encoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. |
| **RoBERTa-large** | Language | Finetune and deploy RoBERTa-large by using PEFT. |
| **XLM-RoBERTa-large** | Language | Finetune and deploy XLM-RoBERTa-large (a multilingual version of RoBERTa) by using PEFT. |
| **Stable Diffusion XL v1.0** | Language, Vision | Deploy Stable Diffusion XL v1.0, which supports text-to-image generation. |
| **Stable Diffusion XL Lightning** | Language, Vision | Deploy Stable Diffusion XL Lightning, a text-to-image generation model. |
| **Stable Diffusion v2.1** | Language, Vision | Finetune and deploy Stable Diffusion v2.1 (supports text-to-image generation) by using Dreambooth. |
| **Stable Diffusion 4x upscaler** | Language, Vision | Deploy Stable Diffusion 4x upscaler, which supports text conditioned image superresolution. |
| **InstructPix2Pix** | Language, Vision | Deploy InstructPix2Pix, which supports image editing by using a text prompt. |
| **Stable Diffusion Inpainting** | Language, Vision | Finetune and deploy Stable Diffusion Inpainting, which supports inpainting a masked image by using a text prompt. |
| **SAM** | Language, Vision | Deploy Segment Anything, which supports zero-shot image segmentation. |
| **Pic2Word Composed Image Retrieval** | Language, Vision | Deploy Pic2Word, which supports multi-modal composed image retrieval. |
| **BLIP2** | Language, Vision | Deploy BLIP2, which supports image captioning and visual-question-answering. |
| **Open-CLIP** | Language, Vision | Finetune and deploy the Open-CLIP, which supports zero-shot classification. |
| **F-VLM** | Language, Vision | Deploy F-VLM, which supports open vocabulary image object detection. |
| **tfhub/EfficientNetV2** | Vision | Finetune and deploy the TensorFlow Vision implementation of the EfficientNetV2 image classification model. |
| **EfficientNetV2 (TIMM)** | Vision | Finetune and deploy the PyTorch implementation of the EfficientNetV2 image classification model. |
| **Proprietary/EfficientNetV2** | Vision | Finetune and deploy the Google proprietary checkpoint of the EfficientNetV2 image classification model. |
| **EfficientNetLite (MediaPipe)** | Vision | Finetune EfficientNetLite image classification model through MediaPipe model maker. |
| **tfvision/vit** | Vision | Finetune and deploy the TensorFlow Vision implementation of the ViT image classification model. |
| **ViT (TIMM)** | Vision | Finetune and deploy the PyTorch implementation of the ViT image classification model. |
| **Proprietary/ViT** | Vision | Finetune and deploy the Google proprietary checkpoint of the ViT image classification model. |
| **Proprietary/MaxViT** | Vision | Finetune and deploy the Google proprietary checkpoint of the MaxViT hybrid (CNN + ViT) image classification model. |
| **ViT (JAX)** | Vision | Finetune and deploy the JAX implementation of the ViT image classification model. |
| **tfvision/SpineNet** | Vision | Finetune and deploy the TensorFlow Vision implementation of the SpineNet object detection model. |
| **Proprietary/Spinenet** | Vision | Finetune and deploy the Google proprietary checkpoint of the SpineNet object detection model. |
| **tfvision/YOLO** | Vision | Finetune and deploy the TensorFlow Vision implementation of the YOLO one-stage object detection model. |
| **Proprietary/YOLO** | Vision | Finetune and deploy the Google proprietary checkpoint of the YOLO one-stage object detection model. |
| **YOLOv8 (Keras)** | Vision | Finetune and deploy the Keras implementation of the YOLOv8 model for object detection. |
| **tfvision/YOLOv7** | Vision | Finetune and deploy YOLOv7 model for object detection. |
| **ByteTrack Video Object Tracking** | Vision | Run batch prediction for video object tracking by using ByteTrack tracker. |
| **ResNeSt (TIMM)** | Vision | Finetune and deploy the PyTorch implementation of the ResNeSt image classification model. |
| **ConvNeXt (TIMM)** | Vision | Finetune and deploy ConvNeXt, a pure convolutional model for image classification inspired by the design of Vision Transformers. |
| **CspNet (TIMM)** | Vision | Finetune and deploy the CSPNet (Cross Stage Partial Network) image classification model. |
| **Inception (TIMM)** | Vision | Finetune and deploy the Inception image classification model. |
| **DeepLabv3+ (with checkpoint)** | Vision | Finetune and deploy the DeepLab-v3 Plus model for semantic image segmentation. |
| **Faster R-CNN (Detectron2)** | Vision | Finetune and deploy the Detectron2 implementation of the Faster R-CNN model for image object detection. |
| **RetinaNet (Detectron2)** | Vision | Finetune and deploy the Detectron2 implementation of the RetinaNet model for image object detection. |
| **Mask R-CNN (Detectron2)** | Vision | Finetune and deploy the Detectron2 implementation of the Mask R-CNN model for image object detection and segmentation. |
| **ControlNet** | Vision | Finetune and deploy the ControlNet text-to-image generation model. |
| **MobileNet (TIMM)** | Vision | Finetune and deploy the PyTorch implementation of the MobileNet image classification model. |
| **MobileNetV2 (MediaPipe)** | Vision | Finetune the MobileNetV2 image classification model by using MediaPipe model maker. |
| **MobileNet-MultiHW-AVG (MediaPipe)** | Vision | Finetune the MobileNet-MultiHW-AVG object detection model by using MediaPipe model maker. |
| **DeiT** | Vision | Finetune and deploy the DeiT (Data-efficient Image Transformers) model for image classification. |
| **BEiT** | Vision | Finetune and deploy the BEiT (Bidirectional Encoder representation from Image Transformers) model for image classification. |
| **Hand Gesture Recognition (MediaPipe)** | Vision | Finetune and deploy on-device the Hand Gesture Recognition models by using MediaPipe. |
| **Average Word Embedding Classifier (MediaPipe)** | Vision | Finetune and deploy on-device the Average Word Embedding Classifier models by using MediaPipe. |
| **MobileBERT Classifier (MediaPipe)** | Vision | Finetune and deploy on-device the MobileBERT Classifier models by using MediaPipe. |
| **MoViNet Video Clip Classification** | Video | Finetune and deploy MoViNet video clip classification models. |
| **MoViNet Video Action Recognition** | Video | Finetune and deploy MoViNet models for action recognition inference. |
| **Stable Diffusion XL LCM** | Vision | Deploy this model which uses the Latent Consistency Model (LCM) to enhance text-to-image generation in Latent Diffusion Models by enabling faster and high-quality image creation with fewer steps. |
| **LLaVA 1.5** | Vision, Language | Deploy LLaVA 1.5 models. |
| **Pytorch-ZipNeRF** | Vision, Video | Train the Pytorch-ZipNeRF model which is a state-of-the-art implementation of the ZipNeRF algorithm in the Pytorch framework, designed for efficient and accurate 3D reconstruction from 2D images. |
| **Mixtral** | Language | Deploy the Mixtral model which is a Mixture of Experts (MoE) large language model (LLM) developed by Mistral AI. |
| **Llama 2 (Quantized)** | Language | Fine-tune & deploy a quantized version of Meta's Llama 2 models. |
| **LaMa (Large Mask Inpainting)** | Vision | Deploy LaMa which uses fast Fourier convolutions (FFCs), a high receptive field perceptual loss and large training masks allows for resolution-robust image inpainting. |
| **AutoGluon** | Tabular | With AutoGluon you can train and deploy high-accuracy machine learning and deep learning models for tabular data. |
| **MaMMUT** | Language, Vision | A vision-encoder and text-decoder architecture for multimodal tasks such as visual question answering, image-text retrieval, text-image retrieval, and generation of multimodal embeddings. |
| **Whisper Large** | Speech | Deploy Whisper Large, OpenAI's state-of-the-art model for automatic speech recognition (ASR). |
